{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tfs\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model  # Import Model from tensorflow.keras.models\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images\n",
    "image = img_to_array(load_img('woman.jpg'))\n",
    "image = np.array(image, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rgb2lab(1.0/255*image)[:,:,0]\n",
    "Y = rgb2lab(1.0/255*image)[:,:,1:]\n",
    "Y /= 128\n",
    "X = X.reshape(1, 400, 400, 1)\n",
    "Y = Y.reshape(1, 400, 400, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(None, None, 1)))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None, None, 1)]   0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, None, None, 8)     80        \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, None, None, 8)     584       \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, None, None, 16)    1168      \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, None, None, 16)    2320      \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, None, None, 32)    4640      \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, None, None, 32)    9248      \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSampli  (None, None, None, 32)    0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, None, None, 32)    9248      \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSampli  (None, None, None, 32)    0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, None, None, 16)    4624      \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSampli  (None, None, None, 16)    0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, None, None, 2)     290       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32202 (125.79 KB)\n",
      "Trainable params: 32202 (125.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(None, None, 1))\n",
    "\n",
    "# Convolutional Layers\n",
    "conv1 = Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)(input_layer)\n",
    "conv2 = Conv2D(8, (3, 3), activation='relu', padding='same')(conv1)\n",
    "conv3 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv2)\n",
    "conv4 = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(conv3)\n",
    "conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv4)\n",
    "conv6 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(conv5)\n",
    "\n",
    "# Upsampling Layers\n",
    "up1 = UpSampling2D((2, 2))(conv6)\n",
    "conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
    "up2 = UpSampling2D((2, 2))(conv7)\n",
    "conv8 = Conv2D(16, (3, 3), activation='relu', padding='same')(up2)\n",
    "up3 = UpSampling2D((2, 2))(conv8)\n",
    "\n",
    "# Final Convolutional Layer\n",
    "output_layer = Conv2D(2, (3, 3), activation='tanh', padding='same')(up3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1032\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0379\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0142\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0092\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.0085\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.0083\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.0081\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 0.0079\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.0076\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21423a1e8b0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, \n",
    "    y=Y,\n",
    "    batch_size=1,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0072\n",
      "0.007248362060636282\n",
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (400,400,2) into shape (2048,1605,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LIKHIT~1\\AppData\\Local\\Temp/ipykernel_6128/948316862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresized_gray_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#cur[:,:,0] = X[0][:,:,0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"img_result.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab2rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"img_gray_version.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlab2rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (400,400,2) into shape (2048,1605,2)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(model.evaluate(X, Y, batch_size=1))\n",
    "output = model.predict(X)\n",
    "output *= 128\n",
    "# Output colorizations\n",
    "cur = np.zeros((2048,1605, 3))\n",
    "gray_image = X[0][:,:,0]\n",
    "resized_gray_image = cv2.resize(gray_image, (1605, 2048))\n",
    "cur[:,:,0] = resized_gray_image\n",
    "#cur[:,:,0] = X[0][:,:,0]\n",
    "cur[:,:,1:] = output[0]\n",
    "imsave(\"img_result.png\", lab2rgb(cur))\n",
    "imsave(\"img_gray_version.png\", rgb2gray(lab2rgb(cur)))\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load the grayscale image\n",
    "gray_image = X[0][:,:,0]\n",
    "\n",
    "# Resize the grayscale image to match the dimensions of cur\n",
    "resized_gray_image = cv2.resize(gray_image, (2048, 1605))\n",
    "\n",
    "# Assign the resized grayscale image to the first channel of cur\n",
    "cur[:,:,0] = resized_gray_image.T  # Transpose the image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assign the resized grayscale image to the first channel of cur\n",
    "cur[:,:,0] = resized_gray_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
